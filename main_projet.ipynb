{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ISyl0dCUpl1M",
        "outputId": "b84a42bd-9b88-4e62-da62-a0c6b06cef7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "import re\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "# -------- Chargement du dataset --------\n",
        "dataset = load_dataset(\"SetFit/amazon_reviews_multi_fr\", split=\"train[:1000]\")\n",
        "df = pd.DataFrame(dataset)\n",
        "\n",
        "# -------- Nettoyage --------\n",
        "def clean_text(text):\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)\n",
        "    text = re.sub(r\"[^a-zàâéèêëîïôùûç0-9\\s]\", \"\", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "\n",
        "df[\"texte_clean\"] = df[\"text\"].apply(clean_text)\n",
        "\n",
        "# -------- Attribution sentiment --------\n",
        "def label_sentiment(label):\n",
        "    if label == 1:\n",
        "        return \"positive\"\n",
        "    elif label == 2:\n",
        "        return \"negative\"\n",
        "    return \"neutral\"\n",
        "\n",
        "df[\"sentiment\"] = df[\"label\"].apply(label_sentiment)\n",
        "\n",
        "# -------- Modèle Mistral --------\n",
        "model_name = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "gen_pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=120\n",
        ")\n",
        "\n",
        "def generer_reponse(texte, sentiment=\"negative\"):\n",
        "    prompt = f\"Le client a écrit : {texte}\\nSentiment : {sentiment}\\nRédige une réponse polie et professionnelle.\"\n",
        "    return gen_pipe(prompt)[0][\"generated_text\"]\n",
        "\n",
        "print(df.head())\n"
      ]
    }
  ]
}