{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ldzcoBypxZPw",
        "outputId": "172d66c4-42a3-4f2f-9e48-93103549494c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing generation.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile generation.py\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "import torch\n",
        "\n",
        "model_name = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "gen_pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=120\n",
        ")\n",
        "\n",
        "def generer_reponse(texte, sentiment=\"negative\"):\n",
        "    prompt = (\n",
        "        f\"Le client a écrit : {texte}\\n\"\n",
        "        f\"Sentiment : {sentiment}\\n\"\n",
        "        f\"Rédige une réponse polie et professionnelle.\"\n",
        "    )\n",
        "    return gen_pipe(prompt)[0][\"generated_text\"]\n"
      ]
    }
  ]
}