# -*- coding: utf-8 -*-
"""generation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1T1aUnu4A4mTEKrlXAReaW4GEc726nZzp
"""

# generation.py

import os

# ============================
# 1. MODE CI / MOCK (important)
# ============================
SKIP_MODEL = os.environ.get("SKIP_MODEL_DOWNLOAD", "false").lower() in ("1", "true", "yes")

if not SKIP_MODEL:
    # ============================
    # Chargement du vrai modèle
    # ============================
    from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
    import torch

    model_name = "mistralai/Mistral-7B-Instruct-v0.3"

    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.float16,
        device_map="auto"
    )

    gen_pipe = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        max_new_tokens=120
    )

# ============================
# 2. Fonction API
# ============================
def generer_reponse(texte, sentiment="negative"):
    """
    Fonction principale : génère une réponse au client.
    - En production : utilise Mistral-7B
    - En CI (tests) : renvoie une réponse MOCK pour éviter le modèle
    """

    # MODE CI : pas de modèle → MOCK
    if SKIP_MODEL:
        return f"[MOCK] Réponse simulée pour CI. Texte='{texte[:30]}...' (sentiment={sentiment})"

    # MODE NORMAL : génération réelle
    prompt = (
        f"Le client a écrit : {texte}\n"
        f"Sentiment : {sentiment}\n"
        f"Rédige une réponse polie et professionnelle."
    )

    return gen_pipe(prompt)[0]["generated_text"]