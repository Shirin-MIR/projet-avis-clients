# -*- coding: utf-8 -*-
"""generation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tKGBzpjm87Zo12jZll9JqgxhdmrdMgm5
"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile generation.py
# from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
# import torch
# 
# model_name = "mistralai/Mistral-7B-Instruct-v0.3"
# 
# tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)
# model = AutoModelForCausalLM.from_pretrained(
#     model_name,
#     torch_dtype=torch.float16,
#     device_map="auto"
# )
# 
# gen_pipe = pipeline(
#     "text-generation",
#     model=model,
#     tokenizer=tokenizer,
#     max_new_tokens=120
# )
# 
# def generer_reponse(texte, sentiment="negative"):
#     prompt = (
#         f"Le client a écrit : {texte}\n"
#         f"Sentiment : {sentiment}\n"
#         f"Rédige une réponse polie et professionnelle."
#     )
#     return gen_pipe(prompt)[0]["generated_text"]
#